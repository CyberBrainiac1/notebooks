{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cded1f5a",
      "metadata": {
        "id": "cded1f5a"
      },
      "source": [
        "# ü§ñ Pranav's Personal AI Assistant - Fine-tuning on Google Colab\n",
        "\n",
        "**Train an AI model on your personal profile, projects, and knowledge!**\n",
        "\n",
        "This notebook trains a **Llama 3.2 3B** model on Pranav's profile information including:\n",
        "- FTC/FRC robotics projects (Evergreen Dragons, Team 2854 Prototypes)\n",
        "- DIY projects (Robotic Hand, Sim Racing Wheel, Robotic Arm)\n",
        "- Technical preferences and skills\n",
        "- General knowledge capabilities\n",
        "\n",
        "**Hardware Requirements:** Runs comfortably on Google Colab Free (T4 GPU: 8GB VRAM, 36GB RAM)\n",
        "\n",
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://unsloth.ai/docs/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c8c9d2",
      "metadata": {
        "id": "80c8c9d2"
      },
      "source": [
        "## üìã Table of Contents\n",
        "1. [Installation](#Installation)\n",
        "2. [Load Model](#Model)\n",
        "3. [Upload Your Dataset](#Dataset)\n",
        "4. [Train](#Train)\n",
        "5. [Test the Model](#Inference)\n",
        "6. [Save & Export](#Save)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575ca6a1",
      "metadata": {
        "id": "575ca6a1"
      },
      "source": [
        "<a name=\"Installation\"></a>\n",
        "## üîß Installation\n",
        "\n",
        "Install Unsloth and required dependencies. This takes about 1-2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d83f8d",
      "metadata": {
        "id": "f6d83f8d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth  # Do this in local & cloud setups\n",
        "else:\n",
        "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
        "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f83564",
      "metadata": {
        "id": "24f83564"
      },
      "source": [
        "<a name=\"Model\"></a>\n",
        "## üß† Load Llama 3.2 3B Model\n",
        "\n",
        "We use **Llama 3.2 3B Instruct** - perfect for 8GB VRAM! It's:\n",
        "- Fast and efficient\n",
        "- Great for conversational AI\n",
        "- Supports 4-bit quantization for low memory usage\n",
        "- Strong general knowledge + ability to learn your specific information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1be6d1",
      "metadata": {
        "id": "3f1be6d1"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # Context window size\n",
        "dtype = None  # Auto-detect: Float16 for T4, Bfloat16 for newer GPUs\n",
        "load_in_4bit = True  # Use 4-bit quantization to fit in 8GB VRAM\n",
        "\n",
        "print(\"üöÄ Loading Llama 3.2 3B Instruct...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c029a2b",
      "metadata": {
        "id": "6c029a2b"
      },
      "source": [
        "## üéØ Add LoRA Adapters\n",
        "\n",
        "LoRA (Low-Rank Adaptation) lets us fine-tune efficiently by only updating 1-10% of parameters.\n",
        "This saves memory and training time while maintaining quality!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fc8453",
      "metadata": {
        "id": "b3fc8453"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,  # LoRA rank - higher = more capacity (using 32 for good quality)\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,  # 0 is optimized\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Saves 30% VRAM!\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters added!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35cd3b38",
      "metadata": {
        "id": "35cd3b38"
      },
      "source": [
        "<a name=\"Dataset\"></a>\n",
        "## üìÇ Upload Your Dataset\n",
        "\n",
        "**Option 1: Upload the dataset file**\n",
        "\n",
        "Run the cell below and upload `pranav_profile_qa.jsonl` when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f90af5f",
      "metadata": {
        "id": "0f90af5f"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"üì§ Upload your pranav_profile_qa.jsonl file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save to a known location\n",
        "dataset_path = \"pranav_profile_qa.jsonl\"\n",
        "if \"pranav_profile_qa.jsonl\" in uploaded:\n",
        "    print(f\"‚úÖ Dataset uploaded successfully!\")\n",
        "    print(f\"   File size: {len(uploaded['pranav_profile_qa.jsonl']) / 1024:.2f} KB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Please upload pranav_profile_qa.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6cdf15",
      "metadata": {
        "id": "7f6cdf15"
      },
      "source": [
        "**Option 2: Download from GitHub**\n",
        "\n",
        "If you have the dataset in a GitHub repo, uncomment and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a1bc22",
      "metadata": {
        "id": "41a1bc22"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/unslothai/notebooks/main/data/pranav_profile_qa.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5e0ddd",
      "metadata": {
        "id": "9c5e0ddd"
      },
      "source": [
        "## üìä Load and Prepare Dataset\n",
        "\n",
        "We'll load the JSONL dataset and convert it to Llama 3's chat format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ef2fe6",
      "metadata": {
        "id": "61ef2fe6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Set up chat template for Llama 3.2\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",  # Llama 3.2 uses 3.1 format\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"üìñ Loading dataset...\")\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(dataset)} training examples\")\n",
        "print(f\"\\nExample fields: {dataset.column_names}\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc1beb2",
      "metadata": {
        "id": "fcc1beb2"
      },
      "outputs": [],
      "source": [
        "# Format function to convert to chat format\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    texts = []\n",
        "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
        "        # Create conversation with system instruction\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": instruction},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "            {\"role\": \"assistant\", \"content\": output}\n",
        "        ]\n",
        "\n",
        "        # Apply chat template\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"üîÑ Formatting dataset for training...\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(\"‚úÖ Dataset formatted!\")\n",
        "print(f\"\\nFormatted example (first 500 chars):\")\n",
        "print(dataset[0][\"text\"][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e8f461",
      "metadata": {
        "id": "a4e8f461"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "## üèãÔ∏è Train the Model\n",
        "\n",
        "Now let's fine-tune! This will take approximately 15-30 minutes depending on dataset size.\n",
        "\n",
        "**Training Settings:**\n",
        "- Batch size: 2 (fits in 8GB VRAM)\n",
        "- Gradient accumulation: 4 steps (effective batch size of 8)\n",
        "- Steps: 300 (increase for more training)\n",
        "- Learning rate: 2e-4 (standard for LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba014b6",
      "metadata": {
        "id": "5ba014b6"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False,  # Can enable for speed with short sequences\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 300,  # Increase to 600+ for stronger learning\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47a91b1c",
      "metadata": {
        "id": "47a91b1c"
      },
      "source": [
        "### Train Only on Assistant Responses\n",
        "\n",
        "We'll mask the user inputs so the model only learns to generate assistant responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0761b340",
      "metadata": {
        "id": "0761b340"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training will focus on assistant responses only!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdd34d5",
      "metadata": {
        "id": "8bdd34d5"
      },
      "outputs": [],
      "source": [
        "# Check memory before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"üéÆ GPU: {gpu_stats.name}\")\n",
        "print(f\"üíæ Max memory: {max_memory} GB\")\n",
        "print(f\"üìä Memory reserved: {start_gpu_memory} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88708c9d",
      "metadata": {
        "id": "88708c9d"
      },
      "outputs": [],
      "source": [
        "# Start training!\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"‚è∞ This will take approximately 15-30 minutes...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efed569a",
      "metadata": {
        "id": "efed569a"
      },
      "outputs": [],
      "source": [
        "# Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(\"üìä Training Statistics:\")\n",
        "print(f\"‚è±Ô∏è  Time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
        "print(f\"üíæ Peak memory: {used_memory} GB ({used_percentage}% of {max_memory} GB)\")\n",
        "print(f\"üéØ Memory for training: {used_memory_for_lora} GB ({lora_percentage}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fdca17d",
      "metadata": {
        "id": "1fdca17d"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "## üéØ Test Your Model!\n",
        "\n",
        "Let's test the trained model with questions about Pranav's profile and projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530bacab",
      "metadata": {
        "id": "530bacab"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# Enable fast inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úÖ Model ready for inference!\")\n",
        "print(\"üéØ Let's test with some questions about Pranav's profile...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b9e43f",
      "metadata": {
        "id": "74b9e43f"
      },
      "source": [
        "### Test 1: FTC Team Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8105fab",
      "metadata": {
        "id": "a8105fab"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What FTC team is Pranav on and what are his leadership goals?\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "print(\"Question: What FTC team is Pranav on and what are his leadership goals?\\n\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 128,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.9\n",
        ")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be665a4",
      "metadata": {
        "id": "1be665a4"
      },
      "source": [
        "### Test 2: DIY Projects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c450124d",
      "metadata": {
        "id": "c450124d"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about Pranav's sim racing steering wheel build.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "print(\"Question: Tell me about Pranav's sim racing steering wheel build.\\n\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 150,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.9\n",
        ")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c874bc",
      "metadata": {
        "id": "95c874bc"
      },
      "source": [
        "### Test 3: Technical Preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581d31e4",
      "metadata": {
        "id": "581d31e4"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What CAD software does Pranav use and what's his coding preference?\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "print(\"Question: What CAD software does Pranav use and what's his coding preference?\\n\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 128,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.9\n",
        ")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1492ce62",
      "metadata": {
        "id": "1492ce62"
      },
      "source": [
        "### Test 4: General Knowledge (Not in Profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ebb34a",
      "metadata": {
        "id": "24ebb34a"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "print(\"Question: What is the capital of France?\\n\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 100,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.9\n",
        ")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820cbf30",
      "metadata": {
        "id": "820cbf30"
      },
      "source": [
        "### üéÆ Try Your Own Questions!\n",
        "\n",
        "Modify the question below to test any topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f79764",
      "metadata": {
        "id": "53f79764"
      },
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è Edit the question here:\n",
        "custom_question = \"What motors does Pranav use in his sim racing wheel?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": custom_question},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "print(f\"Question: {custom_question}\\n\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 150,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.9\n",
        ")\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac94759",
      "metadata": {
        "id": "fac94759"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "## üíæ Save Your Model\n",
        "\n",
        "Now let's save the fine-tuned model! You have several options:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340be1c8",
      "metadata": {
        "id": "340be1c8"
      },
      "source": [
        "### Option 1: Save LoRA Adapters (Small, ~200MB)\n",
        "\n",
        "This saves only the trained adapter weights. You'll need the base model + adapters to run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702f7de3",
      "metadata": {
        "id": "702f7de3"
      },
      "outputs": [],
      "source": [
        "print(\"üíæ Saving LoRA adapters...\")\n",
        "\n",
        "model.save_pretrained(\"pranav_assistant_lora\")\n",
        "tokenizer.save_pretrained(\"pranav_assistant_lora\")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters saved to 'pranav_assistant_lora' folder!\")\n",
        "print(\"üì¶ You can download this folder from the Colab file browser (left sidebar)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff845c57",
      "metadata": {
        "id": "ff845c57"
      },
      "source": [
        "### Option 2: Upload to Hugging Face Hub (Recommended!)\n",
        "\n",
        "Upload to Hugging Face so you can use it anywhere. Get your token at: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8637ee67",
      "metadata": {
        "id": "8637ee67"
      },
      "outputs": [],
      "source": [
        "# Uncomment and fill in your details:\n",
        "# HF_USERNAME = \"your-username\"  # Your Hugging Face username\n",
        "# HF_TOKEN = \"hf_...\"  # Your Hugging Face token\n",
        "\n",
        "# model.push_to_hub(f\"{HF_USERNAME}/pranav-assistant-3b-lora\", token=HF_TOKEN)\n",
        "# tokenizer.push_to_hub(f\"{HF_USERNAME}/pranav-assistant-3b-lora\", token=HF_TOKEN)\n",
        "\n",
        "# print(f\"‚úÖ Model uploaded to: https://huggingface.co/{HF_USERNAME}/pranav-assistant-3b-lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc97f58",
      "metadata": {
        "id": "8dc97f58"
      },
      "source": [
        "### Option 3: Save Merged 16-bit Model (Larger, ~6GB)\n",
        "\n",
        "This merges LoRA weights into the base model for standalone use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a4bbc51",
      "metadata": {
        "id": "9a4bbc51"
      },
      "outputs": [],
      "source": [
        "# Uncomment to save merged model:\n",
        "# print(\"üíæ Saving merged 16-bit model (this may take a few minutes)...\")\n",
        "# model.save_pretrained_merged(\"pranav_assistant_merged_16bit\", tokenizer, save_method=\"merged_16bit\")\n",
        "# print(\"‚úÖ Merged model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e8ec9a",
      "metadata": {
        "id": "b9e8ec9a"
      },
      "source": [
        "### Option 4: Save as GGUF for llama.cpp / Ollama\n",
        "\n",
        "GGUF format works with llama.cpp, Ollama, LM Studio, and other tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e38ea9",
      "metadata": {
        "id": "b7e38ea9"
      },
      "outputs": [],
      "source": [
        "# Uncomment to save as GGUF (Q8_0 quantization):\n",
        "# print(\"üíæ Saving GGUF model...\")\n",
        "# model.save_pretrained_gguf(\"pranav_assistant\", tokenizer, quantization_method=\"q8_0\")\n",
        "# print(\"‚úÖ GGUF model saved!\")\n",
        "\n",
        "# For better compression, try q4_k_m:\n",
        "# model.save_pretrained_gguf(\"pranav_assistant\", tokenizer, quantization_method=\"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6643c4",
      "metadata": {
        "id": "7f6643c4"
      },
      "source": [
        "### üì• Download Your Model\n",
        "\n",
        "To download from Colab:\n",
        "1. Click the folder icon üìÅ in the left sidebar\n",
        "2. Find your saved model folder\n",
        "3. Right-click ‚Üí Download\n",
        "\n",
        "Or use this code to zip and download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5caa8e10",
      "metadata": {
        "id": "5caa8e10"
      },
      "outputs": [],
      "source": [
        "# Uncomment to zip and download:\n",
        "# !zip -r pranav_assistant_lora.zip pranav_assistant_lora/\n",
        "# from google.colab import files\n",
        "# files.download(\"pranav_assistant_lora.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5285dfa2",
      "metadata": {
        "id": "5285dfa2"
      },
      "source": [
        "## üîÑ Reload Your Model Later\n",
        "\n",
        "To use your saved model in a new session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57da6831",
      "metadata": {
        "id": "57da6831"
      },
      "outputs": [],
      "source": [
        "# Uncomment to reload from saved LoRA:\n",
        "# from unsloth import FastLanguageModel\n",
        "#\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"pranav_assistant_lora\",  # Your saved folder\n",
        "#     max_seq_length = 2048,\n",
        "#     dtype = None,\n",
        "#     load_in_4bit = True,\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model)\n",
        "#\n",
        "# # Now you can use it for inference!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a99a2e3",
      "metadata": {
        "id": "1a99a2e3"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully trained a personalized AI assistant! Here's what you accomplished:\n",
        "\n",
        "‚úÖ Fine-tuned Llama 3.2 3B on your personal profile  \n",
        "‚úÖ Trained on 8GB VRAM (Google Colab Free)  \n",
        "‚úÖ Model learns your projects, preferences, and background  \n",
        "‚úÖ Maintains general knowledge capabilities  \n",
        "‚úÖ Saved for future use  \n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "1. **Increase Training**: Change `max_steps` to 600-1000 for stronger learning\n",
        "2. **Add More Data**: Expand your dataset with more details and examples\n",
        "3. **Deploy It**: Use on Ollama, LM Studio, or your own server\n",
        "4. **Share It**: Upload to Hugging Face for others to try\n",
        "\n",
        "### üìö Resources:\n",
        "\n",
        "- **Unsloth Docs**: https://unsloth.ai/docs/\n",
        "- **Discord Community**: https://discord.gg/unsloth\n",
        "- **GitHub**: https://github.com/unslothai/unsloth\n",
        "\n",
        "### üí° Tips for Better Results:\n",
        "\n",
        "- More diverse examples = better generalization\n",
        "- Include both factual Q&A and conversational examples\n",
        "- Test regularly and add edge cases to your dataset\n",
        "- For specialized knowledge, increase training steps\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "\n",
        "  ‚≠êÔ∏è If this helped you, star <a href=\"https://github.com/unslothai/unsloth\">Unsloth on GitHub</a>! ‚≠êÔ∏è\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}